-- No, Really, How Do We Keep Robots From Destroying Humans?
-- B y   t h e   E d i t o r s
-- 2013-07-28T22:00:01Z
-- http://www.bloomberg.com/news/2013-07-28/no-really-how-do-we-keep-robots-from-destroying-humans-.html
In the demilitarized zone dividing
North and South Korea,  SGR-1 robots  are on patrol, equipped with
cameras and radar to detect intruders as well as speakers to
warn them off. If that fails, they also carry machine guns and
grenade launchers.  In the U.S., the  Home Exploring Robotic Butler  can retrieve
a book from a shelf, a meal from a microwave or a drink from the
kitchen. It can even separate an Oreo cookie.  In  Japan , a seal-like robot called Paro provides
companionship for seniors -- and seems to ease the effects of
dementia.  Over the next few decades, robots will become part of
everyday life. But as they grow more sophisticated and
autonomous, they’ll confront situations of cultural and moral
ambiguity that won’t be easily resolved -- situations that
people, over the millennia, have learned to navigate but that
resist codification that machines can easily understand. This
means robots, from the battlefield to the nursing home, will
require more advanced ethical-decision-making abilities. And
humans will need to think through what should happen when they
cause harm.  Three challenges in particular need to be explored.  Robot Warriors  The first and most immediate is in warfare. Some 40
countries are at work on weapons and military equipment that
have some degree of autonomy -- from drones to the  Legged Squad
Support System  -- as are plenty of private companies. The appeal
seems obvious. Unmanned weapons don’t need health insurance or
food or hazard pay. They never lash out in anger, disobey an
order or suffer from post-traumatic stress. And, at least in
theory, they could save the lives of many human soldiers.  At the same time, fully autonomous weapons -- those that
are capable of making their own decisions about whether to
attack or kill, without a human “in the loop” -- make us
deeply uneasy. Only 26 percent of respondents to a  survey  by the
University of Massachusetts Amherst favored their use.  Human
Rights Watch  has asserted that they violate international
humanitarian law and  should be banned .  Yet bans on specific weapons systems -- such as military
airplanes or submarines -- have almost never been effective in
the past. Instead, legal prohibitions and ethical norms have
arisen that effectively limit their use. So a more  promising
approach  might be to adapt existing international law to govern
autonomous technology -- for instance, by requiring that such
weapons, like all others, can’t be used indiscriminately or
cause unnecessary suffering. It may turn out that robotic
weapons are actually better at meeting those requirements than
humans are.  Outside of warfare, robots will confront situations with no
obvious moral resolution. Suppose one is assigned to make sure
your grandmother takes her pills. Then one day she refuses. A
host of quandaries -- from medical ethics to  privacy rights  to
cultural mores -- arise that would be hard enough for a person
to resolve.  Situations like this will demand that engineers  cooperate 
closely with ethicists in designing software, and they will
require much more sophisticated rules than the “ Three Laws of
Robotics ” made famous by Isaac Asimov. Ronald Arkin, of the
Georgia Institute of Technology, has done  pioneering work  on
creating “ ethical governors ” for robots. But we’re a long way
from a satisfactory simulation of morality. Technology companies
would be wise to boost their investment in such research for the
sake of both profits and liability.  Hybrid Liability  Which leads to our third concern. When a robot with some
degree of autonomy unexpectedly harms someone or something -- as
will surely happen -- who’s  liable ? The manufacturer? The
software designer? The owner? To some extent, the existing tort
system can be adapted to help sort things out. One  insightful  
study suggests a hybrid liability system in which robots would
be treated as domesticated animals in cases where their owners
or victims acted negligently, and as commercial products in
cases where the machines were defective.  As robots grow more sophisticated, and people more reliant
on them, another model to consider is the  National Vaccine
Injury Compensation Program . The government could establish a
fund, paid for by a tax on autonomous machines, to compensate
accident victims, thus ensuring that manufacturers won’t fear
rare but very costly lawsuits -- and won’t be discouraged from
inventing new robots -- provided they follow best practices in
designing and marketing them.  In warfare as in ordinary life, when robots cause harm, it
will be critical that the lines of accountability are clear.
Scenarios in which intelligent machines grow self-aware enough
to enslave humanity -- evoked so vividly in movies such as “The
Terminator” -- aren’t plausible. Yet they express an important
 human intuition : There is a danger in ceding too much control to
technology.  This intuition can help guide us into the new robotic era.
But we shouldn’t let it unduly impede a promising new field.
With the right rules in place, the rise of the machines should
be nothing to fear.  To contact the Bloomberg View editorial board:
 view@bloomberg.net . 