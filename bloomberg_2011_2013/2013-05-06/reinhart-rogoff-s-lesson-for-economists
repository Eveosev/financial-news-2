-- Reinhart-Rogoff’s Lesson for Economists
-- B y   B e t s e y   S t e v e n s o n   &   J u s t i n   W o l f e r s
-- 2013-05-06T22:00:00Z
-- http://www.bloomberg.com/news/2013-05-06/reinhart-rogoff-s-lesson-for-economists.html
What lesson can economists draw from
the ruckus over a flaw found in an influential study by two
Harvard University scholars? Our suggestion: Do a better job of
checking one another’s work.  Empirical research has enjoyed an unaccustomed level of
public attention since April 16, when a group of researchers
published a  critique  of work by the economists  Carmen Reinhart 
and  Kenneth Rogoff . The critique  pointed out  a spreadsheet error
-- now famous thanks to the blogosphere and “ The Colbert Report ”
-- in a 2010 study on the relationship between government debt
and economic growth.  Many observers have concluded that the error went
undetected for so long because the research never underwent peer
review, a traditional stop on the way to the coveted goal of
publication in a prestigious journal. But peer review isn’t a
line-by-line error check. It involves a few academics making a
holistic judgment as to whether new research increases our
understanding of the world.  There’s only one reliable way to verify empirical findings:
Try to replicate them. In the narrowest terms, this can mean
taking the author’s data and checking their spreadsheets, as
economists Thomas Herndon, Michael Ash and Robert Pollin did in
their critique of Reinhart and Rogoff. At a broader level,
replication can mean collecting new data, assessing their
reliability and using them to subject a finding to fresh
scrutiny.  Perverse Incentives  Sadly, perverse incentives have made both the narrow and
the broad forms of replication exceedingly rare in economics. As
a result, we don’t actually know how reliable most economic
studies are.  Replication rarely leads to career success. “Ideas” people
-- those exciting scholars generating new insights into how
society functions -- are the stars of the profession. Those who
do the grindingly difficult work of checking whether the stars’
insights are actually true rarely get recognized. Who can name
an economist who achieved fame through replication?  Editors of academic journals prefer to give what scarce
space they have to exciting new ideas, rather than rehashing old
debates. They commonly ignore even clear evidence of errors. In
one case,  according  to the economist Mark Thoma, the flagship
American Economic Review declined to correct a mistake in a
paper written by  Ben S. Bernanke  and  Alan Blinder , even though
the authors acknowledged the error.  For a scholar, replication offers an unappealing bet.
Heads, you discover that the findings of an original study are
largely correct, and no journal will publish your paper because
there’s no interest in learning that something is still true.
Tails, you find a serious flaw, but your results still probably
won’t be published and you’ve earned enemies who may try to land
some reputational punches against you.  Even getting authors to share the data needed to replicate
a study can be a challenge. In one  study , a team of determined
replicators tried to examine 54 articles published in a leading
macroeconomics journal in the 1980s. Many authors never
responded to repeated requests for programs and data. Others
refused or sent raw and often unintelligible computer files.
When replication was possible, it frequently uncovered errors.
All told, the team was able to replicate the findings exactly in
only two articles.  Things have improved somewhat since the 1980s. Some leading
economics journals, such as the American Economic Review,
 require  that authors make their data available. Enforcement,
though, remains  lax . A recent  report  found that only 20 of 39
papers published in the AER could be readily replicated.  Reproducibility Project  The economics profession can do better. Journal editors
should refuse to publish papers until they have been replicated
by a research assistant -- something the  Brookings Papers on
Economic Activity  (of which Wolfers is a co-editor) already
does. Poor replication policies undermine the value of all
published research, because readers can’t differentiate the work
of careful scholars from the sloppier approaches of others.  Other academic disciplines are showing the way. After a
wave of research into psychology studies found some to be
irreproducible and others to be outright  fraudulent , dozens of
psychologists signed on to the  Reproducibility Project , which is
working to systematically replicate important findings. Other
fields are pursuing an “open science” approach, with greater
transparency throughout the research process.  Governments and universities, as major funders of research,
have the power to improve replication standards. Unfortunately,
Congress is moving in the other direction. A new bill drafted by
Representative  Lamar Smith , Republican of  Texas , demands that
the National Science Foundation fund only research that “is not
duplicative of other research projects being funded by the
Foundation or other Federal science agencies.”  It’s important to recognize that the way the Reinhart-Rogoff case unfolded is something of an anomaly in the world of
academic economics. The authors made their raw data available on
the Web, and shared their spreadsheets with their potential
critics. Their statistical methods were simple and largely
transparent. As a result, their error was easy to find and
explain. The broad public interest in their research brought the
debate into the open, where the status hierarchies that afflict
most social science debates are less important.  Most economic research isn’t as conducive to being easily
corrected. Increasingly complex methods, employing gigabytes of
data and reams of computer code, are making errors all the more
likely. The power of incumbents to protect themselves, and the
lack of incentives to unseat them, is weakening the marketplace
of ideas. The gap between evidence and policy is widening as
policy makers and the public learn to distrust statistical
findings.  Perhaps a stronger culture of replication can help close
the gap.  (Betsey Stevenson is an associate professor of public
policy at the  University of Michigan .  Justin Wolfers  is a
professor of public policy and economics at the University of
Michigan, and a nonresident senior fellow of the  Brookings
Institution . Both are Bloomberg View columnists. This is the
last in a series of articles related to the Reinhart-Rogoff
research. Read  part  one and  part two . The opinions expressed are
their own.)  To contact the writers of this article:
Justin Wolfers at  jwolfers@umich.edu  and
Betsey Stevenson at  betseys@umich.edu .  To contact the editor responsible for this article:
Mark Whitehouse at 
 mwhitehouse1@bloomberg.net . 